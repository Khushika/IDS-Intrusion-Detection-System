# -*- coding: utf-8 -*-
"""TRY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H0-kd1IfBKkAilTjCNE8lq7zIwWcxMHC
"""

from google.colab import drive
drive.mount('/content/drive')

"""IMPORTING THE LIBRARIES"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""IMPORT AND LOAD DATASET

attacks_types = {
    'normal': 'normal',
'back': 'dos',
'buffer_overflow': 'u2r',
'ftp_write': 'r2l',
'guess_passwd': 'r2l',
'imap': 'r2l',
'ipsweep': 'probe',
'land': 'dos',
'loadmodule': 'u2r',
'multihop': 'r2l',
'neptune': 'dos',
'nmap': 'probe',
'perl': 'u2r',
'phf': 'r2l',
'pod': 'dos',
'portsweep': 'probe',
'rootkit': 'u2r',
'satan': 'probe',
'smurf': 'dos',
'spy': 'r2l',
'teardrop': 'dos',
'warezclient': 'r2l',
'warezmaster': 'r2l',
}
"""

import pandas as pd

# Load train and test datasets
train_path = "/content/drive/MyDrive/ISS/Train_data.csv"
test_path = "/content/drive/MyDrive/ISS/Test_data.csv"

train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)

"""PREPROCESSING THE DATA"""

# Print types of values in 'class' column
print("\nTypes of values in 'class' column:")
print(train_df['class'].value_counts())

"""VISUALIZING THE DATA"""

import pandas as pd
import matplotlib.pyplot as plt

# Load train and test datasets
train_path = "/content/drive/MyDrive/ISS/Train_data.csv"
test_path = "/content/drive/MyDrive/ISS/Test_data.csv"

train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)

# Display the first few rows of the train dataset
print("Train Dataset:")
print(train_df.head())

# Display the first few rows of the test dataset
print("\nTest Dataset:")
print(test_df.head())

# Plotting histograms for numerical features in the train dataset
train_df.hist(figsize=(10, 8))
plt.suptitle("Histograms of Numerical Features in Train Dataset")
plt.show()

# Plotting a bar plot for categorical features in the train dataset
categorical_cols = train_df.select_dtypes(include='object').columns
for col in categorical_cols:
    train_df[col].value_counts().plot(kind='bar', figsize=(8, 6))
    plt.title(f"Bar Plot of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()

"""REMOVING THE HIGHLY CORRELATED COLOUMNS"""

# List of highly correlated columns to be removed
columns_to_remove = ['num_root', 'srv_serror_rate', 'srv_rerror_rate',
                     'dst_host_srv_serror_rate', 'dst_host_serror_rate',
                     'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',
                     'dst_host_same_srv_rate']

# Remove highly correlated columns from train dataset
train_df.drop(columns=columns_to_remove, inplace=True)

# Remove highly correlated columns from test dataset
test_df.drop(columns=columns_to_remove, inplace=True)

# Print columns of the train dataset
print("Columns of the train dataset after removing highly correlated columns:")
print(train_df.columns)

# Print columns of the test dataset
print("\nColumns of the test dataset after removing highly correlated columns:")
print(test_df.columns)

# Print count of columns after removing highly correlated columns
print("Count of columns after removing highly correlated columns:")
print("Train dataset:", len(train_df.columns))
print("Test dataset:", len(test_df.columns))

"""LABEL ENCODING THE FEATURES"""

from sklearn.preprocessing import LabelEncoder

# Combine train and test datasets for label encoding
combined_df = pd.concat([train_df, test_df], axis=0)

# Convert any numeric values in categorical features to strings
categorical_features = ['protocol_type', 'service', 'flag', 'class']
for feature in categorical_features:
    combined_df[feature] = combined_df[feature].astype(str)

# Perform label encoding for categorical features
label_encoder = LabelEncoder()
for feature in categorical_features:
    combined_df[feature] = label_encoder.fit_transform(combined_df[feature])

# Split back into train and test datasets
train_df = combined_df.iloc[:len(train_df)]
test_df = combined_df.iloc[len(train_df):]

# Confirm the changes
print("Train dataset after label encoding:")
print(train_df.head())

print("\nTest dataset after label encoding:")
print(test_df.head())

"""BUILDING AND TRAINING A NEURAL NETWORK

CNN
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint

# Load the dataset
train_data = train_df.copy()

# Preprocessing
X_train = train_data.drop(columns=['class'])  # Features
y_train = train_data['class']  # Labels

# Encode labels using LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)

# Convert features to appropriate format for CNN
X_train = np.array(X_train).reshape(len(X_train), X_train.shape[1], 1)
y_train = to_categorical(y_train)

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Model Architecture
model = Sequential([
    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),
    MaxPooling1D(pool_size=2),
    Conv1D(64, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(y_train.shape[1], activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val))

# Print training accuracy
train_loss, train_accuracy = model.evaluate(X_train, y_train)
print("Training Accuracy:", train_accuracy)

# Evaluate the model on the test dataset
test_loss, test_accuracy = model.evaluate(X_val, y_val)
print("Test Accuracy:", test_accuracy)

# Save the model
model.save("cnn_model.h5")

"""SNN"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Load the dataset
train_data = train_df.copy()

# Preprocessing
X_train = train_data.drop(columns=['class'])  # Features
y_train = train_data['class']  # Labels

# Encode labels using LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)

# Determine the number of classes in the dataset
num_classes = len(np.unique(y_train))

# Convert features to appropriate format for SNN
X_train = np.array(X_train)
y_train = np.array(y_train)

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Define the SNN model architecture
snn_model = Sequential([
    Flatten(input_shape=(X_train.shape[1],)),  # Flatten the input
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Compile the model
snn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the SNN model
snn_history = snn_model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val))

# Evaluate and print the training accuracy
snn_train_loss, snn_train_accuracy = snn_model.evaluate(X_train, y_train)
print("SNN Training Accuracy:", snn_train_accuracy)

# Evaluate the SNN model on the test dataset
snn_test_loss, snn_test_accuracy = snn_model.evaluate(X_val, y_val)
print("SNN Test Accuracy:", snn_test_accuracy)

# Save the SNN model
snn_model.save("snn_model.h5")

"""DNN"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Load the dataset
train_data = train_df.copy()

# Preprocessing
X_train = train_data.drop(columns=['class'])  # Features
y_train = train_data['class']  # Labels

# Encode labels using LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)

# Determine the number of classes in the dataset
num_classes = len(np.unique(y_train))

# Convert features to appropriate format for DNN
X_train = np.array(X_train)
y_train = np.array(y_train)

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Define the DNN model architecture
dnn_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Compile the model
dnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the DNN model
dnn_history = dnn_model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val))

# Evaluate and print the training accuracy
dnn_train_loss, dnn_train_accuracy = dnn_model.evaluate(X_train, y_train)
print("DNN Training Accuracy:", dnn_train_accuracy)

# Evaluate the DNN model on the test dataset
dnn_test_loss, dnn_test_accuracy = dnn_model.evaluate(X_val, y_val)
print("DNN Test Accuracy:", dnn_test_accuracy)

# Save the DNN model
dnn_model.save("dnn_model.h5")

"""FNN"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Load the dataset
train_data = train_df.copy()

# Preprocessing
X_train = train_data.drop(columns=['class'])  # Features
y_train = train_data['class']  # Labels

# Encode labels using LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)

# Determine the number of classes in the dataset
num_classes = len(np.unique(y_train))

# Convert features to appropriate format for FNN
X_train = np.array(X_train)
y_train = np.array(y_train)

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Define the FNN model architecture
fnn_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Compile the model
fnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the FNN model
fnn_history = fnn_model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val))

# Evaluate and print the training accuracy
fnn_train_loss, fnn_train_accuracy = fnn_model.evaluate(X_train, y_train)
print("FNN Training Accuracy:", fnn_train_accuracy)

# Evaluate the FNN model on the test dataset
fnn_test_loss, fnn_test_accuracy = fnn_model.evaluate(X_val, y_val)
print("FNN Test Accuracy:", fnn_test_accuracy)

# Save the FNN model
fnn_model.save("fnn_model.h5")

"""LSTM"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Load the dataset
train_data = train_df.copy()

# Preprocessing
X_train = train_data.drop(columns=['class'])  # Features
y_train = train_data['class']  # Labels

# Encode labels using LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)

# Determine the number of classes in the dataset
num_classes = len(np.unique(y_train))

# Convert features to appropriate format for LSTM
X_train = np.array(X_train)
X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))  # Reshape for LSTM
y_train = np.array(y_train)

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Define the LSTM model architecture
lstm_model = Sequential([
    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Compile the model
lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the LSTM model
lstm_history = lstm_model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val))

# Evaluate and print the training accuracy
lstm_train_loss, lstm_train_accuracy = lstm_model.evaluate(X_train, y_train)
print("LSTM Training Accuracy:", lstm_train_accuracy)

# Evaluate the LSTM model on the validation dataset
lstm_val_loss, lstm_val_accuracy = lstm_model.evaluate(X_val, y_val)
print("LSTM Validation Accuracy:", lstm_val_accuracy)

# Save the LSTM model
lstm_model.save("lstm_model.h5")

"""#INTEGRATED APPROACH"""

# Check the column names of the training dataset
print(train_df.columns)

# Check the column names of the testing dataset
print(test_df.columns)

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense

# Load the training and testing datasets
train_data = pd.read_csv("/content/drive/MyDrive/ISS/Train_data.csv")
test_data = pd.read_csv("/content/drive/MyDrive/ISS/Test_data.csv")

# Handle missing values if any
train_data.fillna(0, inplace=True)
test_data.fillna(0, inplace=True)

# Check if 'class' column exists in training data
if 'class' in train_data.columns:
    # Remove 'class' column from training data
    X_train = train_data.drop("class", axis=1)
    y_train = train_data["class"]
else:
    X_train = train_data

# Check if 'class' column exists in testing data
if 'class' in test_data.columns:
    # Remove 'class' column from testing data
    X_test = test_data.drop("class", axis=1)
    y_test = test_data["class"]  # Define y_test
else:
    X_test = test_data
    # Define a placeholder y_test since we don't have true labels for prediction
    y_test = np.zeros(len(X_test))  # Create a placeholder array with zeros

# Encode categorical variables
label_encoders = {}
for column in X_train.select_dtypes(include=["object"]).columns:
    label_encoders[column] = LabelEncoder()
    combined_data = pd.concat([X_train[column], X_test[column]], axis=0)
    label_encoders[column].fit(combined_data)
    X_train[column] = label_encoders[column].transform(X_train[column])
    X_test[column] = label_encoders[column].transform(X_test[column])

# Scale numerical features
scaler = StandardScaler()
X_train[X_train.select_dtypes(include=["int64", "float64"]).columns] = scaler.fit_transform(
    X_train[X_train.select_dtypes(include=["int64", "float64"]).columns]
)
X_test[X_test.select_dtypes(include=["int64", "float64"]).columns] = scaler.transform(
    X_test[X_test.select_dtypes(include=["int64", "float64"]).columns]
)

# Define the models
cnn_model = Sequential([
    Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=64, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')
])

snn_model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], 1)),
    LSTM(64),
    Dense(1, activation='sigmoid')
])

dnn_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

fnn_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

lstm_model = Sequential([
    LSTM(128, input_shape=(X_train.shape[1], 1)),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Load pre-trained models if available

# Combine outputs of each model
cnn_preds = cnn_model.predict(X_test.values.reshape(-1, X_test.shape[1], 1))
snn_preds = snn_model.predict(X_test)
dnn_preds = dnn_model.predict(X_test)
fnn_preds = fnn_model.predict(X_test)
lstm_preds = lstm_model.predict(X_test.values.reshape(-1, X_test.shape[1], 1))

# Weighted averaging of predictions
weighted_preds = (0.3 * cnn_preds + 0.2 * snn_preds + 0.2 * dnn_preds + 0.2 * fnn_preds + 0.1 * lstm_preds)

# Assuming you have true labels y_test and predictions ensemble_pred
ensemble_pred = np.round(np.mean(weighted_preds, axis=1)).astype(int)

from sklearn.metrics import accuracy_score

# Calculate the accuracy
accuracy = accuracy_score(y_test, ensemble_pred)
print("Accuracy of the final ensemble model:", accuracy)